{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "840eb94a-22af-48b2-b6d9-9479a92d79a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_patches_streamlit(hdf5_file : Union[str, pathlib.Path], patches_csv: Union[str, pathlib.Path], validation_csv: Union[str, pathlib.Path], train_ratio: float = 0.7, stride: int = 32, output_size=256, always_train_csv: Union[str, bool] = False):\n",
    "\n",
    "    names_list = list(h5py.File(hdf5_file, 'r').keys())\n",
    "    # Set aside the labels you always want the model to be trained on ( e.g. fossils)\n",
    "    if type(always_train_csv) is str:\n",
    "        train_names, val_names = remove_from_validation_set(names_list=names_list, always_train_csv=always_train_csv, train_ratio=train_ratio)\n",
    "    else:\n",
    "        train_names, val_names = separate_names(names_list, train=train_ratio)\n",
    "    #Get patches and validation information for csv\n",
    "    patches = get_patches(hdf5_file=hdf5_file, train_names=train_names, stride=stride, output_size=output_size)\n",
    "\n",
    "    # May have to have a toggle to switch between.\n",
    "    # This is likely not required. For experiments it should be the sk_shuffle so they are consistent.\n",
    "    rand_shuffle(patches)\n",
    "\n",
    "    #Get the information for the validation data\n",
    "    with h5py.File(hdf5_file, 'r') as data_f:\n",
    "        val = [[name, '0', '0', data_f[name]['label'][()].shape[0], data_f[name]['label'][()].shape[1]] for name in val_names]\n",
    "\n",
    "    #Write patches and validation to a csv\n",
    "    df_headers = ['name', 'top', 'left', 'h', 'w']\n",
    "    patches = pd.DataFrame(patches)\n",
    "    patches.columns = df_headers\n",
    "    patches.to_csv(str(patches_csv), index=False)\n",
    "\n",
    "    val = pd.DataFrame(val)\n",
    "    val.columns = df_headers\n",
    "    val.to_csv(str(validation_csv), index=False)\n",
    "\n",
    "    print(\"Generated \"+str(len(patches))+\" patches\")\n",
    "    return val_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3267db9a-b57b-4439-a186-2c2e9a1844f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#GPU device index\n",
    "if torch.cuda.device_count() != 0:\n",
    "    gpu_ID = torch.cuda.get_device_properties(device=0).name\n",
    "\n",
    "#Previous model\n",
    "\n",
    "#Loops\n",
    "batch_size = 32\n",
    "periods = 2\n",
    "num_epochs = 50\n",
    "\n",
    "#Optimizer paramters\n",
    "optimize_method = \"Adam\"\n",
    "optimize_learning_rate = 0.00001\n",
    "optimize_learning_weight = 1\n",
    "\n",
    "#Data path\n",
    "train_data = \"\"\n",
    "model_output = \"C:\\Users\\n.vanderesse\\Desktop\\STAGE_Nolan\\Git\\IA-SeReOs\\RDN_segmentation_container\\MARS\\morphology\\segmentation\\pytorch_segmentation\\model\"\n",
    "\n",
    "#CSV path\n",
    "patch_csv = \"patches.csv\"\n",
    "validation_csv = \"val.csv\"\n",
    "ratio_csv = \"ratios.csv\"\n",
    "\n",
    "\n",
    "model_save_path = 'save_path'\n",
    "save_path = model_save_path.joinpath(sub_save_file)\n",
    "\n",
    "\n",
    "torch.cuda.set_device(gpu_ID)  # '1','0'\n",
    "\n",
    "net = UNet_Light_RDN(n_channels=3, n_classes=3)\n",
    "\n",
    "net.load_state_dict(torch.load(model_path,map_location=torch.device(type='cuda',index=gpu_ID)))\n",
    "   \n",
    "optimizer = AdaBelief(net.parameters(), lr=float(optimize_learning_rate), eps=float(optimize_learning_weight), betas=(0.9, 0.999), weight_decouple=True, rectify=False)\n",
    "\n",
    "#learning rate schedule\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=periods, gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "932e9f41-a1f0-4260-b207-b581c59fdda0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d980816-ff3a-49e6-b0b6-a7a3ac7f99b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils.dataprocess as dp\n",
    "train_transform = transforms.Compose([dp.Augmentation(output_size=config['output_size']),dp.AdjustMask(class_num=3),dp.Normalize(max=255, min=0),dp.ToTensor()])\n",
    "val_transform = transforms.Compose([dp.AdjustMask(class_num=3),dp.Normalize(max=255, min=0),dp.ToTensor()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2971b37d-035f-4428-8848-13c03d8ade07",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'utils'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnn\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctional\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mF\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mRDN_segmentation_container\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mMARS\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstreamlit_apps\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdataprocess\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mdp\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtqdm\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tqdm\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DataLoader\n",
      "File \u001b[1;32m~\\Desktop\\STAGE_Nolan\\Git\\IA-SeReOs\\RDN_segmentation_container\\MARS\\streamlit_apps\\utils\\__init__.py:1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgenerate\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m generate_hdf5\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'utils'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import RDN_segmentation_container.MARS.streamlit_apps.utils.dataprocess as dp\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader\n",
    "from RDN_segmentation_container.MARS.streamlit_apps.utils.losses import DomainEnrichLoss, dice_loss, DiceOverlap, Accuracy\n",
    "\n",
    "\n",
    "\n",
    "bce_losses = nn.BCEWithLogitsLoss()\n",
    "accuracy = Accuracy()\n",
    "\n",
    "def rdn_train(net, optimizer, data_loader, epoch=None, total_epoch=None, use_gpu = False):\n",
    "    if use_gpu:\n",
    "        net.cuda()\n",
    "    else:\n",
    "        net.cpu()\n",
    "\n",
    "    # set data_loader\n",
    "    data_loader1 = data_loader[0]\n",
    "    data_loader2 = data_loader[1]\n",
    "\n",
    "    it = iter(enumerate(data_loader2))\n",
    "    max_batches2 = len(data_loader2.dataset) // data_loader2.batch_size + (1 if (len(data_loader2.dataset) % data_loader2.batch_size) != 0 else 0)\n",
    "\n",
    "    # the epoch message for printing\n",
    "    epoch_print = 'Epoch:'\n",
    "    if epoch is not None:\n",
    "        epoch_print += f'{epoch + 1}'\n",
    "    if total_epoch is not None:\n",
    "        epoch_print += f'/{total_epoch}'\n",
    "    last_batches = 0.0\n",
    "    loss1_sum = 0.0\n",
    "    loss2_sum = 0.0\n",
    "    with tqdm(total=len(data_loader1.dataset), desc=epoch_print, unit=' batches') as pbar:\n",
    "        for i_batches, sample_batched in enumerate(data_loader1):\n",
    "            last_batches = i_batches\n",
    "            i_batches2, sample_batched2 = next(it)\n",
    "            if i_batches2 + 1 >= max_batches2:\n",
    "                it = iter(enumerate(data_loader2))\n",
    "\n",
    "            mask = sample_batched['mask']\n",
    "            image = sample_batched['image']\n",
    "\n",
    "            image2 = sample_batched2['image']\n",
    "            index = sample_batched2['index']\n",
    "\n",
    "            # convert to gpu\n",
    "            if use_gpu:\n",
    "                mask = mask.cuda().long()\n",
    "                image = image.cuda()\n",
    "                image2 = image2.cuda()\n",
    "\n",
    "            # # prediction\n",
    "            net(image2)\n",
    "            \n",
    "            loss1 = DomainEnrichLoss()(net, index)\n",
    "\n",
    "            pred = F.sigmoid(net(image))\n",
    "            mask = dp.create_one_hot(mask)\n",
    "            loss2 = 0.25 * bce_losses(pred, mask) + (1 - 0.25) * dice_loss(pred, mask)\n",
    "\n",
    "            loss = loss2 + 0.0001*loss1\n",
    "            # backward\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Print results\n",
    "            pbar.update(mask.shape[0])\n",
    "            pbar.set_postfix(loss=loss.cpu().data.numpy(),loss1=loss1.cpu().data.numpy(),loss2=loss2.cpu().data.numpy())\n",
    "            loss1_sum = loss1_sum + loss1.cpu().data.numpy()\n",
    "            loss2_sum = loss2_sum + loss2.cpu().data.numpy()\n",
    "\n",
    "        print(f'\\nAverage, loss1: {(loss1_sum / (last_batches + 1)):.6f}, loss2: {(loss2_sum/ (last_batches + 1)):.6f}.')\n",
    "\n",
    "    ...\n",
    "\n",
    "def rdn_val(net, data_set, use_gpu = False, i_epoch = None, class_num = 3):\n",
    "\n",
    "    dice_overlap = DiceOverlap(class_num)\n",
    "    if use_gpu:\n",
    "        net.cuda()\n",
    "    else:\n",
    "        net.cpu()\n",
    "\n",
    "    # check whether net is in train mode or not\n",
    "    origin_is_train_mode = net.training\n",
    "\n",
    "    # change the net to eval mode\n",
    "    if origin_is_train_mode:\n",
    "        net.eval()\n",
    "\n",
    "    # check whether data set is in train mode\n",
    "    data_set.val()\n",
    "\n",
    "    criterion_value_sum = 0.0\n",
    "    data_loader = DataLoader(data_set, batch_size=1, num_workers=0)\n",
    "    dice_overlap_results = 0.0\n",
    "\n",
    "    for i_batches, sample_batched in enumerate(data_loader):\n",
    "        mask = sample_batched['mask']\n",
    "        image = sample_batched['image']\n",
    "\n",
    "        if use_gpu:\n",
    "            mask = mask.cuda()\n",
    "            image = image.cuda()\n",
    "\n",
    "        # prediction\n",
    "        with torch.no_grad():\n",
    "            pred = net(image)\n",
    "            criterion_value_sum += accuracy(pred, mask.long()).cpu().data.numpy()\n",
    "\n",
    "            if dice_overlap is not None:\n",
    "                dice_overlap_results += dice_overlap(pred, mask.long())\n",
    "\n",
    "    criterion_value = criterion_value_sum / len(data_loader.dataset)\n",
    "    dice_overlap_results = dice_overlap_results / len(data_loader.dataset)\n",
    "    for i in range(dice_overlap_results.shape[0]):\n",
    "        print(f'Class: {i:.0f}, Dice Overlap: {dice_overlap_results[i]:.6f}')\n",
    "\n",
    "    if origin_is_train_mode:\n",
    "        net.train()\n",
    "\n",
    "    # print message\n",
    "    if i_epoch is not None:\n",
    "        print(f\"Epoch: {i_epoch + 1}, Accuracy Value: {criterion_value:.6f}\")\n",
    "\n",
    "    return criterion_value, dice_overlap_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0be3532f-922c-446c-ab72-df6255535ef6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'dataset'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mdataset\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgenerate\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_dirt_bone_patches, random_patches\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'dataset'"
     ]
    }
   ],
   "source": [
    "from RDN_segmentation_container.MARS.streamlit_apps.utils.train import rdn_train, rdn_val\n",
    "from RDN_segmentation_container.MARS.streamlit_apps.utils.dataset import HDF52D\n",
    "from RDN_segmentation_container.MARS.streamlit_apps.utils.generate import get_dirt_bone_patches, random_patches\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.utils.data.DataLoader as DataLoader\n",
    "from net import UNet_Light_RDN\n",
    "\n",
    "# training\n",
    "def training(train_patches, ratios, train_transform, val_transform, val_patches, Epoch, batch_size, data_path, use_gpu, optimizer, period, num_classes):\n",
    "    \n",
    "    epoch_count = 0\n",
    "    iteration = 0\n",
    "    \n",
    "    for i_epoch in range(Epoch):\n",
    "        \n",
    "        if i_epoch < period:\n",
    "            dirt_rate = 0.5\n",
    "        elif i_epoch < 2 * period and i_epoch >= period:\n",
    "            dirt_rate = 0.3\n",
    "        elif i_epoch < 3 * period and i_epoch >= 2 * period:\n",
    "            dirt_rate = 0.1\n",
    "        else:\n",
    "            dirt_rate = 0.0\n",
    "\n",
    "        # Domain enrich patches\n",
    "        # Makes a decision about the lowest percent dirt that can be considered for the training.\n",
    "        new_patches = random_patches(dirt_choose_threshold=0.1, dirt_rate=dirt_rate, patches=train_patches, ratios=ratios)\n",
    "\n",
    "        rdn_patches, index = get_dirt_bone_patches(train_patches, ratios)\n",
    "\n",
    "        data_set1 = HDF52D(data_path, new_patches, val_patches,train_transform=train_transform, val_transform=val_transform)\n",
    "\n",
    "        data_set2 = HDF52D(data_path, rdn_patches, val_patches,train_transform=train_transform, val_transform=val_transform, train_idx=index)\n",
    "\n",
    "        train_data_loader = []\n",
    "\n",
    "        current_batch = batch_size\n",
    "\n",
    "\n",
    "        train_data_loader.append(DataLoader(dataset=data_set1, batch_size=current_batch, shuffle=True, num_workers=0))\n",
    "\n",
    "\n",
    "        train_data_loader.append(DataLoader(dataset=data_set2, batch_size=current_batch, shuffle=True, num_workers=0))\n",
    "\n",
    "        print(f\"learning rate {optimizer.param_groups[0]['lr']:.6f}\")\n",
    "\n",
    "        rdn_train(net, optimizer, train_data_loader, epoch=i_epoch, total_epoch=Epoch, use_gpu=use_gpu)\n",
    "        #lr_scheduler.step()\n",
    "\n",
    "        # validating\n",
    "        val_loss, class_val = rdn_val(net, data_set1, use_gpu=use_gpu, i_epoch=i_epoch, class_num=num_classes)\n",
    "\n",
    "        # save model\n",
    "        save_name = f\"Loss-{epoch_count}_{val_loss:.6f}.pth\"\n",
    "        torch.save(net.state_dict(), save_name)\n",
    "        class_val = pd.DataFrame(class_val)\n",
    "        class_val.columns = [\"Class Dice overlap\"]\n",
    "        epoch_count += 1\n",
    "        iteration = np.floor((100 * epoch_count) / int(Epoch))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bb748aa-c658-4141-aa8c-f68fd630427f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
