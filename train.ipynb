{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-15 14:25:09.375763: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2023-12-15 14:25:09.375791: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2023-12-15 14:25:09.375815: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-12-15 14:25:09.382050: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-12-15 14:25:10.218318: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam\n",
    "from torchvision import transforms\n",
    "import albumentations as albu\n",
    "from torch.utils.data import Dataset\n",
    "from typing import Union,List,Tuple\n",
    "import pathlib\n",
    "from timeit import default_timer as timer\n",
    "from torch.utils.data import DataLoader\n",
    "import h5py\n",
    "from tqdm import tqdm\n",
    "import torchvision\n",
    "import os\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.utils import shuffle\n",
    "import time\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rdn_train train the model for one iteration\n",
    "# Input : \n",
    "# net: The neural network model being trained.\n",
    "# optimizer: The optimizer used for updating the model's parameters.\n",
    "# data_loader: DataLoader providing batches of training data.\n",
    "# epoch: Current epoch number (optional).\n",
    "# total_epoch: Total number of epochs (optional).\n",
    "# tensorboard_plot: Boolean flag for visualizing results on TensorBoard (default is False).\n",
    "#\n",
    "# Output :\n",
    "# returns the total number of iterations processed for this epoch (nb_ite + last_batches)\n",
    "\n",
    "def rdn_train(net, optimizer, data_loader, epoch=None, total_epoch=None, tensorboard_plot=False, nb_ite=0):\n",
    "    max_batches1 = len(data_loader.dataset) // data_loader.batch_size + (1 if (len(data_loader.dataset) % data_loader.batch_size) != 0 else 0)\n",
    "\n",
    "    # the epoch message for printing\n",
    "    epoch_print = 'Epoch:'\n",
    "    if epoch is not None:\n",
    "        epoch_print += f'{epoch + 1}'\n",
    "    if total_epoch is not None:\n",
    "        epoch_print += f'/{total_epoch}'\n",
    "    last_batches = 0.0\n",
    "    loss1_sum = 0.0\n",
    "    loss2_sum = 0.0\n",
    "    ite = 0\n",
    "\n",
    "    #Initialize writer to visualize results on TensorBoard\n",
    "    writer = SummaryWriter(runs_dir)\n",
    "    with tqdm(total=len(data_loader.dataset), desc=epoch_print, unit=' batches') as pbar:\n",
    "        for i_batches, sample_batched in enumerate(data_loader):\n",
    "            last_batches = i_batches\n",
    "            mask = sample_batched['mask']\n",
    "            image = sample_batched['image']\n",
    "            #index = sample_batched['index']\n",
    "\n",
    "            # convert to gpu\n",
    "            mask = mask.to(device).long()\n",
    "            image = image.to(device)\n",
    "\n",
    "            # prediction\n",
    "            pred = net(image)\n",
    "            \n",
    "            #loss1 = DomainEnrichLoss()(net, index, mask)\n",
    "            loss1 = torch.Tensor(0)\n",
    "            mask = create_one_hot(mask)\n",
    "            #Visualisation on Tensorboard\n",
    "            if tensorboard_plot and nb_ite+ite == 0:\n",
    "                writer.add_graph(net, image)\n",
    "            if tensorboard_plot and (ite % (max_batches1 // 3) == 0):\n",
    "                if epoch is not None:\n",
    "                    with torch.no_grad():\n",
    "                        pred2 = net(image)\n",
    "                    m2 = mask.argmax(1)\n",
    "                    m2= m2.cpu().squeeze().data.numpy()\n",
    "                    pred2 = pred2.argmax(1)\n",
    "                    pred2 = pred2.cpu().squeeze().data.numpy()\n",
    "                    color_dict = [[0.0], [128.0/255.0], [1]]\n",
    "                    pred_img = torch.empty_like(image).copy_(image)\n",
    "                    mask_img = torch.empty_like(image).copy_(image)\n",
    "                    pred_img.to(device)\n",
    "                    mask_img.to(device)\n",
    "                    for i in range(len(pred_img)):\n",
    "                        for j in range(len(pred_img[i][0])):\n",
    "                            for k in range(len(pred_img[i][0][j])):\n",
    "                                pred_img[i][0][j][k] = color_dict[pred2[i][j][k]][0]\n",
    "                                mask_img[i][0][j][k] = color_dict[m2[i][j][k]][0]\n",
    "\n",
    "                    writer.add_image('input_image', torchvision.utils.make_grid(image),nb_ite + last_batches)\n",
    "                    writer.add_image('prediction_image', torchvision.utils.make_grid(pred_img),nb_ite + last_batches)\n",
    "                    writer.add_image('mask_image', torchvision.utils.make_grid(mask_img),nb_ite + last_batches)\n",
    "\n",
    "            CE_loss = nn.CrossEntropyLoss()\n",
    "            loss2 = CE_loss(pred, mask)\n",
    "            loss2.to(device)\n",
    "\n",
    "\n",
    "            # backward\n",
    "            optimizer.zero_grad()\n",
    "            loss2.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Print results\n",
    "            pbar.update(mask.shape[0])\n",
    "            pbar.set_postfix(loss=loss2.cpu().data.numpy(),loss1=loss1.cpu().data.numpy(),loss2=loss2.cpu().data.numpy())\n",
    "            loss1_sum = loss1_sum + loss1.cpu().data.numpy()\n",
    "            loss2_sum = loss2_sum + loss2.cpu().data.numpy()\n",
    "            #writer.add_scalars('Losses',{'loss':loss1.cpu().data.numpy(),'loss2':loss2.cpu().data.numpy()}, nb_ite + last_batches)\n",
    "            #writer.add_scalars('Average_Losses',{'loss':(loss2_sum / (last_batches + 1)),'loss2':(loss2_sum / (last_batches + 1))}, nb_ite + last_batches)\n",
    "            \n",
    "            ite += 1\n",
    "\n",
    "        print(f'\\nAverage, loss2: {(loss2_sum/ (last_batches + 1)):.6f}.')\n",
    "    writer.close()\n",
    "    return nb_ite + last_batches\n",
    "\n",
    "#rdn_val test the accuracy of the model for the current epoch\n",
    "# Input :\n",
    "# net: The neural network model.\n",
    "# data_set: The dataset for validation.\n",
    "# i_epoch: Current epoch number (optional).\n",
    "# class_num: Number of classes in the dataset (default is 3).\n",
    "#\n",
    "# Output :\n",
    "# returns the average accuracy (criterion_value) and class-wise dice overlap results.\n",
    "\n",
    "def rdn_val(net, data_set, i_epoch = None, class_num = 3):\n",
    "    dice_overlap = DiceOverlap(class_num)\n",
    "\n",
    "    # check whether net is in train mode or not\n",
    "    origin_is_train_mode = net.training\n",
    "\n",
    "    # change the net to eval mode\n",
    "    if origin_is_train_mode:\n",
    "        net.eval()\n",
    "\n",
    "    # check whether data set is in train mode\n",
    "    data_set.val()\n",
    "\n",
    "    criterion_value_sum = 0.0\n",
    "    data_loader = DataLoader(data_set, batch_size=1, num_workers=0)\n",
    "    dice_overlap_results = 0.0\n",
    "\n",
    "    for i_batches, sample_batched in enumerate(data_loader):\n",
    "        mask = sample_batched['mask']\n",
    "        image = sample_batched['image']\n",
    "\n",
    "        mask = mask.to(device)\n",
    "        image = image.to(device)\n",
    "\n",
    "        # prediction\n",
    "        with torch.no_grad():\n",
    "            if image.shape==(1,1,256,256):\n",
    "                pred = net(image)\n",
    "                criterion_value_sum += accuracy(pred, mask.long()).cpu().data.numpy()\n",
    "\n",
    "                if dice_overlap is not None:\n",
    "                    dice_overlap_results += dice_overlap(pred, mask.long())\n",
    "\n",
    "    criterion_value = criterion_value_sum / len(data_loader.dataset)\n",
    "    dice_overlap_results = dice_overlap_results / len(data_loader.dataset)\n",
    "\n",
    "    for i in range(dice_overlap_results.shape[0]):\n",
    "        print(f'Class: {i:.0f}, Dice Overlap: {dice_overlap_results[i]:.6f}')\n",
    "\n",
    "    if origin_is_train_mode:\n",
    "        net.train()\n",
    "\n",
    "    # print message\n",
    "    if i_epoch is not None:\n",
    "        print(f\"Epoch: {i_epoch + 1}, Accuracy Value: {criterion_value:.6f}\")\n",
    "        writer = SummaryWriter(runs_dir)\n",
    "        writer.add_scalars('Dice Overlap',{'Air':dice_overlap_results[0],'Dirt':dice_overlap_results[1],'Bone':dice_overlap_results[2]}, i_epoch)\n",
    "        writer.close()\n",
    "    return criterion_value, dice_overlap_results\n",
    "\n",
    "class DiceOverlap():\n",
    "\n",
    "    def __init__(self, class_num):\n",
    "        self.len = class_num\n",
    "\n",
    "    def __call__(self, input, target):\n",
    "        input = F.sigmoid(input)\n",
    "        input = torch.max(input, 1)[1]\n",
    "\n",
    "        dice = []\n",
    "\n",
    "        for i in range(self.len):\n",
    "            sub_target = torch.zeros(target.shape).cuda()\n",
    "            sub_target[target == i] = 1\n",
    "            sub_input = torch.zeros(input.shape).cuda()\n",
    "            sub_input[input == i] = 1\n",
    "\n",
    "            tp_idx = target == i\n",
    "\n",
    "            eps = 0.0001\n",
    "            tp = torch.sum(sub_input[tp_idx] == sub_target[tp_idx])\n",
    "            fn = torch.sum(sub_input != sub_target)\n",
    "            tp = tp.float()\n",
    "            fn = fn.float()\n",
    "            result = (2*tp + eps) / (2*tp + fn + eps)\n",
    "            dice.append(result.cpu().data.numpy())\n",
    "\n",
    "        return np.asarray(dice)\n",
    "    \n",
    "class Accuracy():\n",
    "\n",
    "    def __call__(self, input, target, **kwargs):\n",
    "\n",
    "        input = torch.max(input, 1)[1]\n",
    "        size = 1\n",
    "        for i in range(len(input.shape)):\n",
    "            size = size * input.shape[i]\n",
    "        return torch.sum(input == target).float() / size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get_minimum_dirt_patches create a subset of initial patches focusing on patches containing dirt (depending on dirt_rate)\n",
    "# Input :\n",
    "# dirt_choose_threshold: A threshold value for the dirt ratio to determine whether a patch is considered as a dirt patch.\n",
    "# dirt_rate: The desired proportion of dirt patches in the selected subset.\n",
    "# patches: A NumPy array containing information about patches (name, top, left, height, width).\n",
    "# ratios: A NumPy array containing ratios for each patch (class ratios).\n",
    "#\n",
    "# Output :\n",
    "# returns a subset of initial patches\n",
    "def get_minimum_dirt_patches(dirt_choose_threshold: float, dirt_rate: float, patches: np.array, ratios:np.array):\n",
    "    # get ratios\n",
    "    ratios = np.array(ratios)\n",
    "    #get index that would sort ratios by decreasing order\n",
    "    ratios_idx = np.argsort(-ratios, axis=0)\n",
    "\n",
    "    # ratios dimension is n_patches x n_classes\n",
    "    # get the second column of ratio_idx which is the dirt index sorted by decreasing order\n",
    "    dirt_idx = ratios_idx[:, 1]\n",
    "\n",
    "    # get only the patches that dirt ratio is > dirt_choose_threshold\n",
    "    last_idx = 0\n",
    "    for i in range(dirt_idx.shape[0]):\n",
    "        dirt_ratio = ratios[dirt_idx[i], 1]\n",
    "        if dirt_ratio < dirt_choose_threshold:\n",
    "            last_idx = i\n",
    "            break\n",
    "    \n",
    "    #indexes of wanted dirt_patches\n",
    "    dirt_patches_idx = dirt_idx[0:last_idx]\n",
    "    #indexes of other patches\n",
    "    rest_idx = dirt_idx[last_idx:-1]\n",
    "\n",
    "    \n",
    "    if not (dirt_rate == 0):\n",
    "        rest_num = round(((last_idx - 1) / dirt_rate) * (1 - dirt_rate))\n",
    "        if rest_num > rest_idx.shape[0]:\n",
    "            rest_num = rest_idx.shape[0]\n",
    "    else:\n",
    "        rest_num = rest_idx.shape[0]\n",
    "    \n",
    "    #Getting picking other patches\n",
    "    random_idx = np.random.choice(rest_idx.shape[0], size=rest_num, replace=False)\n",
    "    non_dirt_patches_idx = rest_idx[random_idx]\n",
    "\n",
    "    # Getting the final patches\n",
    "    patches_idx = np.concatenate((dirt_patches_idx, non_dirt_patches_idx), axis=0)\n",
    "    new_patches = np.asarray(patches)[patches_idx, :].tolist()\n",
    "\n",
    "    new_patches = shuffle(new_patches)\n",
    "    new_patches = [[name, int(top), int(left), int(h), int(w)] for [name, top, left, h, w] in new_patches]\n",
    "    return new_patches\n",
    "\n",
    "#get_dirt_bone_patches create a subset of initial patches focusing on patches containing air (depending on air_rate)\n",
    "# Input :\n",
    "# patches: A NumPy array containing information about patches (name, top, left, height, width).\n",
    "# ratios: A NumPy array containing ratios for each patch (class ratios).\n",
    "# air_rate: The desired proportion of air patches in the selected subset.\n",
    "#\n",
    "# Output :\n",
    "# returns a list of extracted patches based on the specified conditions and an index representing the length of this list\n",
    "def get_dirt_bone_patches(patches: np.array, ratios:np.array, air_rate: float):\n",
    "    #get ratios \n",
    "    ratios = np.array(ratios)\n",
    "    #get indices that would sort ratios by decreasing order\n",
    "    ratios_idx = np.argsort(-ratios, axis=0) \n",
    "\n",
    "    # get the second column of ratio_idx which is the dirt index sorted by decreasing order\n",
    "    dirt_idx = ratios_idx[:, 1]\n",
    "    #get patches \n",
    "    patches = np.asarray(patches)\n",
    "\n",
    "    #get ratios and patches sorted by decreasing dirt ratios \n",
    "    ratios_sort = ratios[dirt_idx, :]\n",
    "    patches_sort = patches[dirt_idx, :]\n",
    "    \n",
    "    dirt_patches = []\n",
    "    bone_patches = []\n",
    "    \n",
    "    while (len(dirt_patches) < 128 or len(bone_patches) < 128) and air_rate < 1: #in the case that we have not enough patches \n",
    "        dirt_patches = []\n",
    "        bone_patches = []\n",
    "        #get patches that contains significant differences between dirt and bone ratios (>0.1)\n",
    "        for idx in range(ratios_sort.shape[0]):\n",
    "            if (ratios_sort[idx, 0] < air_rate):\n",
    "                if (ratios_sort[idx, 1] - ratios_sort[idx, 2] > 0.15):\n",
    "                    dirt_patches.append(patches_sort[idx, :].tolist())\n",
    "                elif (ratios_sort[idx, 2] - ratios_sort[idx, 1] > 0.15):\n",
    "                    bone_patches.append(patches_sort[idx, :].tolist())\n",
    "            else:\n",
    "                pass\n",
    "        air_rate += 0.1\n",
    "\n",
    "    dirt_len = len(dirt_patches)\n",
    "    bone_len = len(bone_patches)\n",
    "\n",
    "    bone_index = [1 for i in range(bone_len)]\n",
    "    dirt_index = [0 for i in range(dirt_len)]\n",
    "    \n",
    "    dirt_patches = shuffle(dirt_patches)\n",
    "    bone_patches = shuffle(bone_patches)\n",
    "\n",
    " \n",
    "    \n",
    "    print(f\"There are {bone_len} bone and {dirt_len} dirt patches in the training data...\")\n",
    "\n",
    "    end_idx = dirt_len if dirt_len < bone_len else bone_len\n",
    "    #get the same quantity of dirt and bone patches\n",
    "    new_patches = []\n",
    "    for patch in dirt_patches:\n",
    "        new_patches.append(patch)\n",
    "    for patch in bone_patches:\n",
    "        new_patches.append(patch)\n",
    "    d_index = len(new_patches)\n",
    "\n",
    "    #This is the use of sklearn shuffle, which can't be replaced by the random shuffle\n",
    "    #new_patches, d_index = shuffle(new_patches, d_index)\n",
    "    #new_patches = [[name, int(top), int(left), int(h), int(w)] for [name, top, left, h, w] in new_patches]\n",
    "    #d_index = [int(idx) for idx in d_index]\n",
    "    \n",
    "    \n",
    "    return new_patches, d_index\n",
    "\n",
    "#slide_windows generate a list of patches based on a sliding window approach over an input image or data.\n",
    "# Input :\n",
    "# name : A string representing the name or identifier for the image or data.\n",
    "# shape : A tuple representing the shape (height, width) of the input image or data.\n",
    "# output_size (int): An integer or tuple representing the size of the output patches. Default is set to (128, 128).\n",
    "# stride (int): An integer or tuple representing the stride of the sliding window. Default is set to 32.\n",
    "#\n",
    "# Output :\n",
    "#  A list of lists, where each inner list contains information about a patch\n",
    "def slide_windows(name, shape, output_size=128, stride=32):\n",
    "    output_size = (output_size, output_size)\n",
    "    strides = (stride, stride)\n",
    "\n",
    "    patches_list = []\n",
    "    idx = 0\n",
    "    while idx * strides[0] + output_size[0] <= shape[0]:\n",
    "        top = idx * strides[0]\n",
    "        j = 0\n",
    "        while j * strides[1] + output_size[1] <= shape[1]:\n",
    "            left = j * strides[1]\n",
    "            patches_list.append([name, top, left,output_size[0], output_size[1]])\n",
    "            j += 1\n",
    "\n",
    "        if j * strides[1] < shape[1]:\n",
    "            left = shape[1] - output_size[1]\n",
    "            patches_list.append([name, top, left, output_size[0], output_size[1]])\n",
    "        idx += 1\n",
    "\n",
    "    if idx * strides[0] < shape[0]:\n",
    "        top = shape[0] - output_size[0]\n",
    "        j = 0\n",
    "        while j * strides[1] + output_size[1] <= shape[1]:\n",
    "            left = j * strides[1]\n",
    "            patches_list.append([name, top, left, output_size[0], output_size[1]])\n",
    "            j += 1\n",
    "\n",
    "        if j * strides[1] < shape[1]:\n",
    "            left = shape[1] - output_size[1]\n",
    "            patches_list.append([name, top, left, output_size[0], output_size[1]])\n",
    "    return patches_list\n",
    "\n",
    "#get_patches generate patches from a collection of images\n",
    "# Input :\n",
    "# data: A dictionary where keys represent image names, and values are the corresponding images (arrays).\n",
    "# stride: An integer representing the stride of the sliding window. Default is set to 32.\n",
    "# output_size: An integer representing the size of the output patches. Default is set to 256.\n",
    "#\n",
    "# Output :\n",
    "#  A list of lists, where each inner list contains information about a patch\n",
    "def get_patches(data, stride: int = 32, output_size: int = 256):\n",
    "    patches = []\n",
    "    # Iterate for each image\n",
    "    for name in (data.keys()):\n",
    "        shape = data[name].shape\n",
    "        patches += slide_windows(name, shape, output_size=output_size, stride=stride)\n",
    "    shuffle(patches)\n",
    "        # I wrote out the before and after and did a comparison and there are no differences.\n",
    "        #This section appears to simply cast the first two items in patches as strings.\n",
    "        # for idx in range(len(patches)):\n",
    "        #     for j in range(len(patches[idx])):\n",
    "        #         patches[idx][j] = str(patches[idx][j])\n",
    "    return patches\n",
    "\n",
    "#generate_ratios calculates ratios for each patch based on the pixel values in the corresponding labeled masks\n",
    "# Input :\n",
    "# patches: A list of lists, where each inner list contains information about a patch (name, top, left, height, width)\n",
    "# class_num: An integer representing the number of classes in the labeled masks. Default is set to 3.\n",
    "#\n",
    "# Output:\n",
    "# A list of lists, where each inner list contains class ratios for a patch.\n",
    "def generate_ratios(patches, class_num=3):\n",
    "    #initialize ratios to 0\n",
    "    ratios = []\n",
    "    for [name, top, left, h, w] in patches:\n",
    "        mask = plt.imread(label_dir+\"/\"+name)[top: top+h, left: left+w]\n",
    "        mask = adjustMask(mask, class_num)\n",
    "        size = 1.0\n",
    "        for idx in range(len(mask.shape)):\n",
    "            size *= mask.shape[idx]\n",
    "\n",
    "        #Get ratio for the patch\n",
    "        ratio = []\n",
    "        for idx in range(class_num):\n",
    "            ratio.append(np.sum(mask == idx)/size) \n",
    "\n",
    "        ratios.append(ratio)\n",
    "    return ratios"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_patches(patches):\n",
    "\n",
    "    if isinstance(patches, str):\n",
    "        return np.array(pd.read_csv(patches, header=0)).tolist()\n",
    "    else:\n",
    "        return patches\n",
    "\n",
    "class HDF52D(Dataset):\n",
    "\n",
    "    # dataset for segmentation used\n",
    "    def __init__(self, train_patches,val_patches, train_transform=None, val_transform=None, train_idx = None):\n",
    "\n",
    "        self.patches = {'train': load_patches(train_patches),\n",
    "                        'val': load_patches(val_patches)}\n",
    "\n",
    "        self.transforms = {'train': train_transform,\n",
    "                           'val': val_transform}\n",
    "\n",
    "        self.train_idx = load_patches(train_idx)\n",
    "\n",
    "        self.mode = 'train'\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        [name, top, left, h, w] = self.patches[self.mode][idx]\n",
    "\n",
    "        image = input_list[name][int(top):int(top) + int(h), int(left):int(left) + int(w) ]\n",
    "        mask = label_list[name][int(top):int(top) + int(h), int(left):int(left) + int(w)]\n",
    "        sample = {'image': image, 'mask': mask}\n",
    "\n",
    "        if self.transforms[self.mode] is not None:\n",
    "            sample = self.transforms[self.mode](sample)\n",
    "            \n",
    "        #if self.train_idx is not None and self.mode == 'train':\n",
    "        #    sample['index'] = self.train_idx[idx]\n",
    "        return sample\n",
    "\n",
    "\n",
    "    def train(self):\n",
    "        self.mode = 'train'\n",
    "\n",
    "    def val(self):\n",
    "        self.mode = 'val'\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.patches[self.mode])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_one_hot(mask, num_classes = 3):\n",
    "    one_hot_mask = torch.zeros([mask.shape[0],\n",
    "                                num_classes,\n",
    "                                mask.shape[1],\n",
    "                                mask.shape[2]],\n",
    "                               dtype=torch.float32)\n",
    "    if mask.is_cuda:\n",
    "        one_hot_mask = one_hot_mask.cuda()\n",
    "    one_hot_mask = one_hot_mask.scatter(1, mask.long().data.unsqueeze(1), 1.0)\n",
    "\n",
    "    return one_hot_mask\n",
    "\n",
    "def adjustMask(mask, class_num):\n",
    "\n",
    "    interval = int(256.0 / class_num)\n",
    "\n",
    "    # Color_Dict must be a numpy type\n",
    "    # mask.shape must be a H x W x C\n",
    "    # do not have channel dimensions\n",
    "    if len(mask.shape) == 2:\n",
    "        new_mask = np.zeros((mask.shape[0], mask.shape[1]), dtype=np.longlong)\n",
    "        for i in range(class_num):\n",
    "            if i <= class_num - 2:\n",
    "                new_mask[(mask >= i*interval) & (mask < (i+1) * interval)] = i\n",
    "            else:\n",
    "                new_mask[i*interval <= mask] = i\n",
    "        return new_mask\n",
    "\n",
    "class AdjustMask(object):\n",
    "    def __init__(self, class_num = 3):\n",
    "        self.class_num = class_num\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        sample['mask'] = adjustMask(sample['mask'], self.class_num)\n",
    "        return sample\n",
    "\n",
    "class ToTensor(object):\n",
    "\n",
    "    def __init__(self, if_multi_img=False):\n",
    "        self.if_multi_img = if_multi_img\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        image, mask = sample['image'], sample['mask']\n",
    "\n",
    "        # swap color axis because\n",
    "        # numpy image: H x W x C\n",
    "        # torch image: C x H x W\n",
    "\n",
    "        if not self.if_multi_img:\n",
    "            if len(image.shape) == 2:\n",
    "                image = np.expand_dims(image, axis=2)\n",
    "            image = image.transpose((2, 0, 1))\n",
    "        else:\n",
    "            if len(image.shape) == 3:\n",
    "                image = np.expand_dims(image, axis=3)\n",
    "\n",
    "            image = image.transpose((0, 3, 1, 2))\n",
    "\n",
    "        sample['image'] = torch.from_numpy(image)\n",
    "        sample['mask'] = torch.from_numpy(mask)\n",
    "\n",
    "        if 'weights' in sample:\n",
    "            sample['weights'] = torch.from_numpy(sample['weights'])\n",
    "        if 'ratio' in sample:\n",
    "            sample['ratio'] = torch.from_numpy(sample['ratio'])\n",
    "        return sample\n",
    "\n",
    "class Normalize(object):\n",
    "    def __init__(self, max=255.0, min=0.0, tg_max=1.0, tg_min=0.0):\n",
    "        self.max = max\n",
    "        self.min = min\n",
    "        self.tg_max = tg_max\n",
    "        self.tg_min = tg_min\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        image = sample['image'].astype('float32')\n",
    "        image = self.tg_min + ((image - self.min)*(self.tg_max - self.tg_min)) / (self.max - self.min)\n",
    "        sample['image'] = image\n",
    "        return sample\n",
    "\n",
    "class Augmentation(object):\n",
    "\n",
    "    def __init__(self, output_size=256):\n",
    "        self.aug = albu.Compose([\n",
    "            albu.OneOf([\n",
    "                albu.HorizontalFlip(p=1),\n",
    "                albu.VerticalFlip(p=1),   \n",
    "                albu.Compose([\n",
    "                    albu.HorizontalFlip(p=1),\n",
    "                    albu.VerticalFlip(p=1), \n",
    "                ])\n",
    "            ], p=0.75),\n",
    "            # albu.OneOf([\n",
    "            # albu.RandomContrast(),\n",
    "            # albu.RandomGamma(),\n",
    "            # albu.RandomBrightness(),\n",
    "            # ], p=0.5),\n",
    "            # albu.OneOf([\n",
    "            # albu.ElasticTransform(alpha=60, sigma=120 * 0.05, alpha_affine=120 * 0.03),\n",
    "            # albu.GridDistortion(),\n",
    "            # albu.OpticalDistortion(distort_limit=2, shift_limit=0.5),\n",
    "            # ], p=0.),5\n",
    "            albu.augmentations.geometric.rotate.RandomRotate90(p=1),\n",
    "            albu.Resize(output_size, output_size, always_apply=True),\n",
    "        ])\n",
    "    def __call__(self,sample):\n",
    "        augmented = self.aug(image=sample['image'], mask=sample['mask'])\n",
    "        sample['image'] = augmented['image']\n",
    "        sample['mask'] = augmented['mask']\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UNet-parts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DomainEnrich(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.domain_enrich = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.domain_enrich(x)\n",
    "\n",
    "\n",
    "class DoubleConv(nn.Module):\n",
    "    \"\"\"(convolution => [BN] => ReLU) * 2\"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.double_conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.double_conv(x)\n",
    "\n",
    "\n",
    "class Down(nn.Module):\n",
    "    \"\"\"Downscaling with maxpool then double conv\"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.maxpool_conv = nn.Sequential(\n",
    "            nn.MaxPool2d(2),\n",
    "            DoubleConv(in_channels, out_channels)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.maxpool_conv(x)\n",
    "\n",
    "\n",
    "class Up(nn.Module):\n",
    "    \"\"\"Upscaling then double conv\"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, bilinear=True):\n",
    "        super().__init__()\n",
    "\n",
    "        # if bilinear, use the normal convolutions to reduce the number of channels\n",
    "        if bilinear:\n",
    "            self.up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
    "        else:\n",
    "            self.up = nn.ConvTranspose2d(in_channels // 2, in_channels // 2, kernel_size=2, stride=2)\n",
    "\n",
    "        self.conv = DoubleConv(in_channels, out_channels)\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        x1 = self.up(x1)\n",
    "        # input is CHW\n",
    "        diffY = torch.tensor([x2.size()[2] - x1.size()[2]])\n",
    "        diffX = torch.tensor([x2.size()[3] - x1.size()[3]])\n",
    "\n",
    "        x1 = F.pad(x1, [diffX // 2, diffX - diffX // 2,\n",
    "                        diffY // 2, diffY - diffY // 2])\n",
    "        # if you have padding issues, see\n",
    "        # https://github.com/HaiyongJiang/U-Net-Pytorch-Unstructured-Buggy/commit/0e854509c2cea854e247a9c615f175f76fbb2e3a\n",
    "        # https://github.com/xiaopeng-liao/Pytorch-UNet/commit/8ebac70e633bac59fc22bb5195e513d5832fb3bd\n",
    "        x = torch.cat([x2, x1], dim=1)\n",
    "        return self.conv(x)\n",
    "\n",
    "\n",
    "class OutConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(OutConv, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UNet Light RDN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNet_Light_RDN(nn.Module):\n",
    "    def __init__(self, n_channels, n_classes, bilinear=True):\n",
    "        super(UNet_Light_RDN, self).__init__()\n",
    "        self.n_channels = n_channels\n",
    "        self.n_classes = n_classes\n",
    "        self.bilinear = bilinear\n",
    "\n",
    "        #self.rdn1 = DomainEnrich_Block(n_channels, 8)\n",
    "        #self.rdn2 = DomainEnrich_Block(n_channels, 8)\n",
    "\n",
    "        #self.inc = DoubleConv(17, 32)\n",
    "        \n",
    "        self.inc = DoubleConv(1, 32) #only unet\n",
    "        \n",
    "        self.down1 = Down(32, 64)\n",
    "        self.down2 = Down(64, 128)\n",
    "        self.down3 = Down(128, 256)\n",
    "        self.down4 = Down(256, 256)\n",
    "        self.up1 = Up(512, 128, bilinear)\n",
    "        self.up2 = Up(256, 64, bilinear)\n",
    "        self.up3 = Up(128, 32, bilinear)\n",
    "        self.up4 = Up(64, 32, bilinear)\n",
    "        self.outc = OutConv(32, n_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # identity = x\n",
    "        # self.x_rdn1 = self.rdn1(x)\n",
    "        # self.x_rdn2 = self.rdn2(x)\n",
    "        # # self.x_rdn2 = self.rdn2(self.x_rdn1)\n",
    "        # x1 = self.inc(torch.cat((self.x_rdn2, self.x_rdn1, identity), 1))\n",
    "        \n",
    "        x1 = self.inc(x) #only unet\n",
    "        \n",
    "        x2 = self.down1(x1)\n",
    "        x3 = self.down2(x2)\n",
    "        x4 = self.down3(x3)\n",
    "        x5 = self.down4(x4)\n",
    "        x = self.up1(x5, x4)\n",
    "        x = self.up2(x, x3)\n",
    "        x = self.up3(x, x2)\n",
    "        x = self.up4(x, x1)\n",
    "        logits = self.outc(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Put the device on GPU if possible to train the model faster than with CPU\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "batch_size = 32\n",
    "period_size = 8\n",
    "weight_decay = 0.1\n",
    "epsilon = 10**(-8)\n",
    "epochs_num = 20\n",
    "learning_rate = 10**(-3)\n",
    "n_channel = 1\n",
    "class_num = 3\n",
    "\n",
    "label_dir = \"label\"\n",
    "input_dir = \"input\"\n",
    "runs_dir = os.path.join(\"runs\", datetime.datetime.now().strftime('%Y-%m-%d%H-%M-%S'))\n",
    "\n",
    "stride = 32\n",
    "train_size = 0.7\n",
    "\n",
    "output = 256\n",
    "\n",
    "accuracy = Accuracy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_list = os.listdir(label_dir)\n",
    "\n",
    "label_list = {name:plt.imread(label_dir+\"/\"+name) for name in name_list}\n",
    "input_list = {name:plt.imread(input_dir+\"/\"+name) for name in name_list}\n",
    "\n",
    "#Divide each unsegmented picture into smaller picture as the training set\n",
    "train_patches = get_patches(input_list, stride=stride, output_size=output)\n",
    "#Same thing for the labelised pictures as the label set\n",
    "val_patches = get_patches(label_list, stride=stride, output_size=output)\n",
    "#Calculate the % of air, bones and dirt for each patch of the training set\n",
    "ratios = generate_ratios(val_patches, class_num=class_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "UNet_Light_RDN(\n",
       "  (inc): DoubleConv(\n",
       "    (double_conv): Sequential(\n",
       "      (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU(inplace=True)\n",
       "      (3): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (down1): Down(\n",
       "    (maxpool_conv): Sequential(\n",
       "      (0): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (1): DoubleConv(\n",
       "        (double_conv): Sequential(\n",
       "          (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "          (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (5): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (down2): Down(\n",
       "    (maxpool_conv): Sequential(\n",
       "      (0): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (1): DoubleConv(\n",
       "        (double_conv): Sequential(\n",
       "          (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "          (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (5): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (down3): Down(\n",
       "    (maxpool_conv): Sequential(\n",
       "      (0): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (1): DoubleConv(\n",
       "        (double_conv): Sequential(\n",
       "          (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "          (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (5): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (down4): Down(\n",
       "    (maxpool_conv): Sequential(\n",
       "      (0): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (1): DoubleConv(\n",
       "        (double_conv): Sequential(\n",
       "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "          (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (5): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (up1): Up(\n",
       "    (up): Upsample(scale_factor=2.0, mode='bilinear')\n",
       "    (conv): DoubleConv(\n",
       "      (double_conv): Sequential(\n",
       "        (0): Conv2d(512, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "        (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (up2): Up(\n",
       "    (up): Upsample(scale_factor=2.0, mode='bilinear')\n",
       "    (conv): DoubleConv(\n",
       "      (double_conv): Sequential(\n",
       "        (0): Conv2d(256, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "        (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (up3): Up(\n",
       "    (up): Upsample(scale_factor=2.0, mode='bilinear')\n",
       "    (conv): DoubleConv(\n",
       "      (double_conv): Sequential(\n",
       "        (0): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "        (3): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (up4): Up(\n",
       "    (up): Upsample(scale_factor=2.0, mode='bilinear')\n",
       "    (conv): DoubleConv(\n",
       "      (double_conv): Sequential(\n",
       "        (0): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "        (3): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (outc): OutConv(\n",
       "    (conv): Conv2d(32, 3, kernel_size=(1, 1), stride=(1, 1))\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = UNet_Light_RDN(n_channels=n_channel, n_classes=class_num)\n",
    "net.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = Adam(net.parameters(),\n",
    "                      lr=float(learning_rate),\n",
    "                      eps=float(epsilon),\n",
    "                      betas=(0.9, 0.999),\n",
    "                      weight_decay=weight_decay)\n",
    "\n",
    "#learning rate schedule\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=period_size, gamma=0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create train transform\n",
    "train_transform = transforms.Compose([Augmentation(output_size=64), #config['output_size']\n",
    "                                                    AdjustMask(class_num=class_num),\n",
    "                                                    Normalize(max=255, min=0),\n",
    "                                                    ToTensor()])\n",
    "\n",
    "val_transform = transforms.Compose([AdjustMask(class_num=class_num),\n",
    "                                                    Normalize(max=255, min=0),\n",
    "                                                    ToTensor()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch progress:\n",
      "Progress training 20 epochs...\n",
      "Epoch 1 of 20\n",
      "There are 219 bone and 0 dirt patches in the training data...\n",
      "learning rate 0.001000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:1/20:   0%|          | 0/219 [00:00<?, ? batches/s]/tmp/ipykernel_11507/1028449713.py:63: TracerWarning: torch.tensor results are registered as constants in the trace. You can safely ignore this warning if you use this function to create tensors out of constant variables that would be the same every time you call this function. In any other case, this might cause the trace to be incorrect.\n",
      "  diffY = torch.tensor([x2.size()[2] - x1.size()[2]])\n",
      "/tmp/ipykernel_11507/1028449713.py:64: TracerWarning: torch.tensor results are registered as constants in the trace. You can safely ignore this warning if you use this function to create tensors out of constant variables that would be the same every time you call this function. In any other case, this might cause the trace to be incorrect.\n",
      "  diffX = torch.tensor([x2.size()[3] - x1.size()[3]])\n",
      "Epoch:1/20: 100%|██████████| 219/219 [00:18<00:00, 11.66 batches/s, loss=0.45994166, loss1=[], loss2=0.45994166]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average, loss2: 0.645850.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 50\u001b[0m\n\u001b[1;32m     46\u001b[0m nb_ite \u001b[38;5;241m=\u001b[39m rdn_train(net, optimizer, train_data_loader, epoch\u001b[38;5;241m=\u001b[39mi_epoch, total_epoch\u001b[38;5;241m=\u001b[39mepochs_num, tensorboard_plot\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, nb_ite\u001b[38;5;241m=\u001b[39mnb_ite)\n\u001b[1;32m     47\u001b[0m \u001b[38;5;66;03m#lr_scheduler.step()\u001b[39;00m\n\u001b[1;32m     48\u001b[0m \n\u001b[1;32m     49\u001b[0m \u001b[38;5;66;03m# validating\u001b[39;00m\n\u001b[0;32m---> 50\u001b[0m val_loss, class_val \u001b[38;5;241m=\u001b[39m \u001b[43mrdn_val\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnet\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_set\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mi_epoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclass_num\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclass_num\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     52\u001b[0m class_val \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(class_val)\n\u001b[1;32m     53\u001b[0m class_val\u001b[38;5;241m.\u001b[39mcolumns \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mClass Dice overlap\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "Cell \u001b[0;32mIn[2], line 133\u001b[0m, in \u001b[0;36mrdn_val\u001b[0;34m(net, data_set, i_epoch, class_num)\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m    132\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m image\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;241m==\u001b[39m(\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m256\u001b[39m,\u001b[38;5;241m256\u001b[39m):\n\u001b[0;32m--> 133\u001b[0m         pred \u001b[38;5;241m=\u001b[39m \u001b[43mnet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    134\u001b[0m         criterion_value_sum \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m accuracy(pred, mask\u001b[38;5;241m.\u001b[39mlong())\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[1;32m    136\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m dice_overlap \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/Documents/ProjetSemestre/SEREOS/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/ProjetSemestre/SEREOS/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[7], line 39\u001b[0m, in \u001b[0;36mUNet_Light_RDN.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     37\u001b[0m x5 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdown4(x4)\n\u001b[1;32m     38\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mup1(x5, x4)\n\u001b[0;32m---> 39\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mup2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx3\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     40\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mup3(x, x2)\n\u001b[1;32m     41\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mup4(x, x1)\n",
      "File \u001b[0;32m~/Documents/ProjetSemestre/SEREOS/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/ProjetSemestre/SEREOS/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[6], line 66\u001b[0m, in \u001b[0;36mUp.forward\u001b[0;34m(self, x1, x2)\u001b[0m\n\u001b[1;32m     63\u001b[0m diffY \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor([x2\u001b[38;5;241m.\u001b[39msize()[\u001b[38;5;241m2\u001b[39m] \u001b[38;5;241m-\u001b[39m x1\u001b[38;5;241m.\u001b[39msize()[\u001b[38;5;241m2\u001b[39m]])\n\u001b[1;32m     64\u001b[0m diffX \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor([x2\u001b[38;5;241m.\u001b[39msize()[\u001b[38;5;241m3\u001b[39m] \u001b[38;5;241m-\u001b[39m x1\u001b[38;5;241m.\u001b[39msize()[\u001b[38;5;241m3\u001b[39m]])\n\u001b[0;32m---> 66\u001b[0m x1 \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mpad(x1, [\u001b[43mdiffX\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m, diffX \u001b[38;5;241m-\u001b[39m diffX \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m,\n\u001b[1;32m     67\u001b[0m                 diffY \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m, diffY \u001b[38;5;241m-\u001b[39m diffY \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m])\n\u001b[1;32m     68\u001b[0m \u001b[38;5;66;03m# if you have padding issues, see\u001b[39;00m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;66;03m# https://github.com/HaiyongJiang/U-Net-Pytorch-Unstructured-Buggy/commit/0e854509c2cea854e247a9c615f175f76fbb2e3a\u001b[39;00m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;66;03m# https://github.com/xiaopeng-liao/Pytorch-UNet/commit/8ebac70e633bac59fc22bb5195e513d5832fb3bd\u001b[39;00m\n\u001b[1;32m     71\u001b[0m x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([x2, x1], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/Documents/ProjetSemestre/SEREOS/lib/python3.11/site-packages/torch/_tensor.py:40\u001b[0m, in \u001b[0;36m_handle_torch_function_and_wrap_type_error_to_not_implemented.<locals>.wrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m has_torch_function(args):\n\u001b[1;32m     39\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(wrapped, args, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m---> 40\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m     42\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mNotImplemented\u001b[39m\n",
      "File \u001b[0;32m~/Documents/ProjetSemestre/SEREOS/lib/python3.11/site-packages/torch/_tensor.py:942\u001b[0m, in \u001b[0;36mTensor.__floordiv__\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m    940\u001b[0m \u001b[38;5;129m@_handle_torch_function_and_wrap_type_error_to_not_implemented\u001b[39m\n\u001b[1;32m    941\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__floordiv__\u001b[39m(\u001b[38;5;28mself\u001b[39m, other):\n\u001b[0;32m--> 942\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloor_divide\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mother\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epoch_count = 0\n",
    "#st.write(f\"TensorBoard is availaible, run this following command in a terminal : tensorboard --logdir=runs\")\n",
    "print(f\"Epoch progress:\")\n",
    "print(f\"Progress training {epochs_num} epochs...\")\n",
    "total_timer = timer()\n",
    "iteration = 0\n",
    "nb_ite = 0\n",
    "#subprocess.call('echo \"TensorBoard available, run this command to enable it : tensorboard --logdir=runs\"', shell=True)\n",
    "for i_epoch in range(epochs_num):\n",
    "    print(f\"Epoch {epoch_count + 1} of {epochs_num}\")\n",
    "    \n",
    "    if i_epoch < period_size:\n",
    "        #dirt_rate = 0.5\n",
    "        air_rate = 0.1\n",
    "    elif i_epoch < 2 * period_size and i_epoch >= period_size:\n",
    "        #dirt_rate = 0.3\n",
    "        air_rate = 0.2\n",
    "    elif i_epoch < 3 * period_size and i_epoch >= 2 * period_size:\n",
    "        #dirt_rate = 0.1\n",
    "        air_rate = 0.4\n",
    "    else:\n",
    "        #dirt_rate = 0.0\n",
    "        air_rate = 0.5\n",
    "    \n",
    "    #Get patches \n",
    "    patches = get_minimum_dirt_patches(dirt_choose_threshold=0.1, dirt_rate=0, patches=train_patches, ratios=ratios)\n",
    "    \n",
    "    DEB_patches, index = get_dirt_bone_patches(train_patches, ratios, air_rate)\n",
    "    \n",
    "    data_set = HDF52D(patches, val_patches, train_transform=train_transform, val_transform=val_transform)\n",
    "    \n",
    "    DEB_data_set = HDF52D(DEB_patches, val_patches, train_transform=train_transform, val_transform=val_transform, train_idx=index)\n",
    "\n",
    "    current_batch = int(batch_size)\n",
    "    \n",
    "    train_data_loader = DataLoader(dataset=DEB_data_set, batch_size=current_batch, shuffle=True, num_workers=0)\n",
    "    \n",
    "    \n",
    "                            \n",
    "    # train_data_loader.append(DataLoader(dataset=training_data_set,\n",
    "    #                                     batch_size=current_batch,\n",
    "    #                                     shuffle=True,\n",
    "    #                                     num_workers=0))\n",
    "                            \n",
    "    print(f\"learning rate {optimizer.param_groups[0]['lr']:.6f}\")\n",
    "    nb_ite = rdn_train(net, optimizer, train_data_loader, epoch=i_epoch, total_epoch=epochs_num, tensorboard_plot=True, nb_ite=nb_ite)\n",
    "    #lr_scheduler.step()\n",
    "    \n",
    "    # validating\n",
    "    val_loss, class_val = rdn_val(net, data_set, i_epoch=i_epoch, class_num=class_num)\n",
    "    \n",
    "    class_val = pd.DataFrame(class_val)\n",
    "    class_val.columns = [\"Class Dice overlap\"]\n",
    "    print(class_val)\n",
    "    epoch_count += 1\n",
    "    iteration = np.floor((100 * epoch_count) / int(epochs_num))\n",
    "\n",
    "# save model\n",
    "torch.save(net.state_dict(), \"RDN.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
